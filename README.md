# MLLM Auto-Labeling for Images & Videos

**Leverage Multimodal Large Language Models (MLLMs) to automatically label your image and video datasets.**

Generate high-quality bounding box annotations using state-of-the-art vision-language models like GPT-4V, Claude 3.5 Sonnet, and Qwen-VL. Save 80%+ annotation time while maintaining 85-95% accuracy.

Integrate with Label Studio for human review and collaborative annotation workflows.

---

## ğŸ“ Project Structure

```
video-autolabeling-pipeline/
â”œâ”€â”€ README.md              # Main documentation
â”œâ”€â”€ LICENSE                # Open source license
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ docs/                  # ğŸ“š Documentation
â”‚   â”œâ”€â”€ å¿«é€Ÿå¼€å§‹.md        # Quick start guide (Chinese)
â”‚   â”œâ”€â”€ QWEN_GUIDE.md      # Qwen-VL detailed guide
â”‚   â”œâ”€â”€ AUTO_LABELING_GUIDE.md  # VLM auto-labeling guide
â”‚   â””â”€â”€ YOLO_GUIDE.md      # YOLO local labeling guide
â”œâ”€â”€ scripts/               # ğŸ”§ Core scripts
â”‚   â”œâ”€â”€ image_auto_labeling.py     # Image auto-labeling
â”‚   â”œâ”€â”€ video_auto_labeling.py     # Video auto-labeling
â”‚   â”œâ”€â”€ yolo_auto_labeling.py      # YOLO labeling
â”‚   â”œâ”€â”€ quick_yolo_label.sh        # YOLO quick labeling script
â”‚   â”œâ”€â”€ visualize_result.py        # Visualize labeling results
â”‚   â”œâ”€â”€ test_qwen_api.py           # Test Qwen API
â”‚   â””â”€â”€ start_label_studio.sh      # Start Label Studio
â”œâ”€â”€ templates/             # ğŸ“‹ Labeling templates
â”œâ”€â”€ data/                  # ğŸ“¹ Data files (examples)
â””â”€â”€ labels/                # ğŸ·ï¸ Labeling results (output)
```

---

## ğŸ“ Getting Started

**First time user?** â†’ Check **[QUICKSTART.md](QUICKSTART.md)** for complete tutorial (10 mins setup)

---

## ğŸš€ Quick Start

**Test with an image (fastest):**

```bash
# 1. Set API Key (choose one)
export DASHSCOPE_API_KEY="your-qwen-key"        # Qwen (recommended for China)
export ANTHROPIC_API_KEY="your-claude-key"     # Claude (recommended for international)

# 2. Label an image
python3 scripts/image_auto_labeling.py your-image.jpg --provider qwen --visualize

# View the labeled result with bounding boxes instantly!
```

> ğŸ’¡ For detailed steps, see **[QUICKSTART.md](QUICKSTART.md)** or **[å¿«é€Ÿå¼€å§‹.md](docs/å¿«é€Ÿå¼€å§‹.md)** (Chinese)

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ¤– **MLLM Auto-Labeling** | Leverage GPT-4V, Claude 3.5 Sonnet, Qwen-VL to auto-generate labels |
| ğŸ“¹ **Video Frame Labeling** | Smart sampling strategies for efficient video annotation |
| ğŸ–¼ï¸ **Image Object Detection** | Single-shot bounding box generation for images |
| âš¡ **Save 80%+ Time** | AI generates initial labels, humans only review and refine |
| ğŸ¯ **High Accuracy** | Claude: 90-95%, Qwen: 85-90%, YOLO: 80-85% |
| ğŸŒ **China-Friendly** | Qwen-VL support, no VPN required |
| ğŸ”§ **Production Ready** | Label Studio integration, batch processing, visualization tools |

---

## ğŸ“š Documentation

**For Beginners:**
- **[QUICKSTART.md](QUICKSTART.md)** ğŸ”° **Start Here** - Complete tutorial, 10-min setup
- **[å¿«é€Ÿå¼€å§‹.md](docs/å¿«é€Ÿå¼€å§‹.md)** â­ Quick Start - Three ways to get started (Chinese)

**Advanced Guides:**
- **[QWEN_GUIDE.md](docs/QWEN_GUIDE.md)** - Qwen-VL detailed guide (recommended for users in China)
- **[AUTO_LABELING_GUIDE.md](docs/AUTO_LABELING_GUIDE.md)** - VLM auto-labeling with GPT-4V, Claude, etc.
- **[YOLO_GUIDE.md](docs/YOLO_GUIDE.md)** - YOLO local labeling (free, offline)

---

## ğŸ¯ Use Cases

- ğŸš— **Autonomous Driving**: Vehicle, pedestrian, traffic sign detection
- ğŸ­ **Industrial QA**: Defect detection, product classification
- ğŸ¥ **Medical Imaging**: Lesion annotation, organ segmentation
- ğŸ“¦ **E-commerce**: Product recognition, shelf monitoring
- ğŸ¥ **Video Analytics**: Action recognition, object tracking

---

## ğŸ’¡ Contributing

We welcome Issues and Pull Requests! If you have questions or suggestions, please contact us on GitHub.

## ğŸ“„ License

This project is licensed under the [LICENSE](LICENSE) file.
